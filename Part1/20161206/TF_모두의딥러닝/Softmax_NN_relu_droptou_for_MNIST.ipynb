{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Softmax classifier (like logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting ./data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting ./data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./MNIST_data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_epoch = 25\n",
    "learning_rate = 0.01\n",
    "batch_size = 100\n",
    "\n",
    "# Graph Input\n",
    "X = tf.placeholder(\"float\", [None, 784]) # 784개의 feature (28 x 28)\n",
    "Y = tf.placeholder(\"float\", [None, 10]) # 1 부터 10\n",
    "\n",
    "# Set model weight\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "activation = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cost function (Cross entropy : 실제값(Y)과 log예측값(activation)의 곱을 합한 후 평균을 냄) \n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(activation), reduction_indices=1))\n",
    "\n",
    "# optimization\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0001 cost= 1.185057776\n",
      "Epoch:  0002 cost= 0.664978850\n",
      "Epoch:  0003 cost= 0.552604859\n",
      "Epoch:  0004 cost= 0.499552738\n",
      "Epoch:  0005 cost= 0.465079360\n",
      "Epoch:  0006 cost= 0.442369879\n",
      "Epoch:  0007 cost= 0.425785417\n",
      "Epoch:  0008 cost= 0.411793717\n",
      "Epoch:  0009 cost= 0.402091410\n",
      "Epoch:  0010 cost= 0.391296188\n",
      "Epoch:  0011 cost= 0.385442796\n",
      "Epoch:  0012 cost= 0.377984825\n",
      "Epoch:  0013 cost= 0.372287661\n",
      "Epoch:  0014 cost= 0.367212594\n",
      "Epoch:  0015 cost= 0.362602765\n",
      "Epoch:  0016 cost= 0.358736135\n",
      "Epoch:  0017 cost= 0.354563447\n",
      "Epoch:  0018 cost= 0.351663312\n",
      "Epoch:  0019 cost= 0.348035226\n",
      "Epoch:  0020 cost= 0.345456941\n",
      "Epoch:  0021 cost= 0.342695041\n",
      "Epoch:  0022 cost= 0.341055348\n",
      "Epoch:  0023 cost= 0.337562731\n",
      "Epoch:  0024 cost= 0.335623243\n",
      "Epoch:  0025 cost= 0.333540952\n",
      "Optimization Finished\n",
      "Accuracy:  0.9135\n"
     ]
    }
   ],
   "source": [
    "# Launch\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Training\n",
    "    for epoch in range(training_epoch):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size) # 55000개 training data\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(100) # 1회에 100개씩 550번 loop\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={X : batch_xs, Y : batch_ys})\n",
    "            \n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "        if epoch % 1 == 0:\n",
    "            print \"Epoch: \", \"%04d\" % (epoch + 1), \"cost=\", \"{:.9f}\".format(avg_cost)\n",
    "        \n",
    "    print \"Optimization Finished\"\n",
    "    \n",
    "    # Test\n",
    "    correct_prediction = tf.equal(tf.argmax(activation, 1), tf.argmax(Y, 1))\n",
    "    \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print \"Accuracy: \", accuracy.eval({X : mnist.test.images, Y : mnist.test.labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32),\n",
       " array([[ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.next_batch(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameter\n",
    "learning_rate = 0.001\n",
    "training_epoch = 25\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Graph input\n",
    "X = tf.placeholder(\"float\", [None, 784])\n",
    "Y = tf.placeholder(\"float\", [None, 10])\n",
    "\n",
    "# Layer and bias\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "L1 = tf.nn.relu(tf.add(tf.matmul(X, W1), b1))\n",
    "L2 = tf.nn.relu(tf.add(tf.matmul(L1, W2), b2))\n",
    "hypothesis = tf.add(tf.matmul(L2, W3), b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cost and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(hypothesis, Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialization\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0001 cost= 190.389436864\n",
      "Epoch:  0002 cost= 42.198810425\n",
      "Epoch:  0003 cost= 26.685083371\n",
      "Epoch:  0004 cost= 18.405697818\n",
      "Epoch:  0005 cost= 13.401345751\n",
      "Epoch:  0006 cost= 9.991915153\n",
      "Epoch:  0007 cost= 7.508097746\n",
      "Epoch:  0008 cost= 5.585075273\n",
      "Epoch:  0009 cost= 4.168195659\n",
      "Epoch:  0010 cost= 3.183739734\n",
      "Epoch:  0011 cost= 2.362985620\n",
      "Epoch:  0012 cost= 1.798196025\n",
      "Epoch:  0013 cost= 1.357462142\n",
      "Epoch:  0014 cost= 1.064521588\n",
      "Epoch:  0015 cost= 0.867181389\n",
      "Epoch:  0016 cost= 0.654093022\n",
      "Epoch:  0017 cost= 0.543513392\n",
      "Epoch:  0018 cost= 0.542527065\n",
      "Epoch:  0019 cost= 0.530578956\n",
      "Epoch:  0020 cost= 0.459948440\n",
      "Epoch:  0021 cost= 0.352804120\n",
      "Epoch:  0022 cost= 0.259783851\n",
      "Epoch:  0023 cost= 0.382489484\n",
      "Epoch:  0024 cost= 0.309532927\n",
      "Epoch:  0025 cost= 0.336732629\n",
      "Optimization Finished\n",
      "Accuracy: 0.9548\n"
     ]
    }
   ],
   "source": [
    "# Launch\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Training\n",
    "    for epoch in range(training_epoch):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size) # 55000개 training data\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(100) # 1회에 100개씩 550번 loop\n",
    "            # Fit training using batch data\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={X : batch_xs, Y : batch_ys})\n",
    "            # average loss\n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "        if epoch % 1 == 0:\n",
    "            print \"Epoch: \", \"%04d\" % (epoch + 1), \"cost=\", \"{:.9f}\".format(avg_cost)\n",
    "        \n",
    "    print \"Optimization Finished\"\n",
    "    \n",
    "    # Test\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print \"Accuracy:\", accuracy.eval({X : mnist.test.images, Y : mnist.test.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NN with xavier initialization (초기 가중치 설정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xavier_init(n_inputs, n_outputs, uniform = True):\n",
    "    if uniform:\n",
    "        init_range = tf.sqrt(6.0 / (n_inputs + n_outputs))\n",
    "        return tf.random_uniform_initializer(-init_range, init_range)\n",
    "    else:\n",
    "        stddev = tf.sqrt(3.0 / (n_inputs + n_outputs))\n",
    "        return tf.truncated_normal_initializer(stddev=stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameter\n",
    "learning_rate = 0.001\n",
    "training_epoch = 25\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Graph input\n",
    "X = tf.placeholder(\"float\", [None, 784])\n",
    "Y = tf.placeholder(\"float\", [None, 10])\n",
    "\n",
    "# Layer and bias\n",
    "W1 = tf.get_variable(\"W1\", shape = [784, 256], initializer=xavier_init(784, 256))\n",
    "W2 = tf.get_variable(\"W2\", shape = [256, 256], initializer=xavier_init(256, 256))\n",
    "W3 = tf.get_variable(\"W3\", shape = [256, 10], initializer=xavier_init(256, 10))\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([256]))\n",
    "b2 = tf.Variable(tf.zeros([256]))\n",
    "b3 = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "L1 = tf.nn.relu(tf.add(tf.matmul(X, W1), b1))\n",
    "L2 = tf.nn.relu(tf.add(tf.matmul(L1, W2), b2))\n",
    "hypothesis = tf.add(tf.matmul(L2, W3), b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cost and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(hypothesis, Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0001 cost= 0.302407114\n",
      "Epoch:  0002 cost= 0.113428493\n",
      "Epoch:  0003 cost= 0.076841262\n",
      "Epoch:  0004 cost= 0.053509536\n",
      "Epoch:  0005 cost= 0.039907459\n",
      "Epoch:  0006 cost= 0.030712249\n",
      "Epoch:  0007 cost= 0.024475898\n",
      "Epoch:  0008 cost= 0.020319543\n",
      "Epoch:  0009 cost= 0.016330425\n",
      "Epoch:  0010 cost= 0.012699563\n",
      "Epoch:  0011 cost= 0.013715739\n",
      "Epoch:  0012 cost= 0.009314998\n",
      "Epoch:  0013 cost= 0.011381943\n",
      "Epoch:  0014 cost= 0.011401653\n",
      "Epoch:  0015 cost= 0.008828260\n",
      "Epoch:  0016 cost= 0.008933662\n",
      "Epoch:  0017 cost= 0.008461802\n",
      "Epoch:  0018 cost= 0.007110051\n",
      "Epoch:  0019 cost= 0.005589009\n",
      "Epoch:  0020 cost= 0.009963807\n",
      "Epoch:  0021 cost= 0.004982289\n",
      "Epoch:  0022 cost= 0.007009625\n",
      "Epoch:  0023 cost= 0.010225282\n",
      "Epoch:  0024 cost= 0.005035763\n",
      "Epoch:  0025 cost= 0.004123504\n",
      "Optimization Finished\n",
      "Accuracy: 0.9766\n"
     ]
    }
   ],
   "source": [
    "# Launch\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Training\n",
    "    for epoch in range(training_epoch):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size) # 55000개 training data\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(100) # 1회에 100개씩 550번 loop\n",
    "            # Fit training using batch data\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={X : batch_xs, Y : batch_ys})\n",
    "            # average loss\n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "        if epoch % 1 == 0:\n",
    "            print \"Epoch: \", \"%04d\" % (epoch + 1), \"cost=\", \"{:.9f}\".format(avg_cost)\n",
    "        \n",
    "    print \"Optimization Finished\"\n",
    "    \n",
    "    # Test\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print \"Accuracy:\", accuracy.eval({X : mnist.test.images, Y : mnist.test.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. More deep and dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Parameter\n",
    "learning_rate = 0.001\n",
    "training_epoch = 25\n",
    "batch_size = 100\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xavier_init(n_inputs, n_outputs, uniform = True):\n",
    "    if uniform:\n",
    "        init_range = tf.sqrt(6.0 / (n_inputs + n_outputs))\n",
    "        return tf.random_uniform_initializer(-init_range, init_range)\n",
    "    else:\n",
    "        stddev = tf.sqrt(3.0 / (n_inputs + n_outputs))\n",
    "        return tf.truncated_normal_initializer(stddev=stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Graph input\n",
    "X = tf.placeholder(\"float\", [None, 784])\n",
    "Y = tf.placeholder(\"float\", [None, 10])\n",
    "dropout_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "# Layer and bias\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 500], initializer=xavier_init(784,500))\n",
    "W2 = tf.get_variable(\"W2\", shape=[500, 256], initializer=xavier_init(500,256))\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 128], initializer=xavier_init(256,128))\n",
    "W4 = tf.get_variable(\"W4\", shape=[128, 64], initializer=xavier_init(128,64))\n",
    "W5 = tf.get_variable(\"W5\", shape=[64, 10], initializer=xavier_init(64,10))\n",
    "\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([500]))\n",
    "b2 = tf.Variable(tf.zeros([256]))\n",
    "b3 = tf.Variable(tf.zeros([128]))\n",
    "b4 = tf.Variable(tf.zeros([64]))\n",
    "b5 = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "dropout_rate = tf.placeholder(\"float\")\n",
    "_L1 = tf.nn.relu(tf.add(tf.matmul(X, W1), b1))\n",
    "L1 = tf.nn.dropout(_L1, dropout_rate)\n",
    "_L2 = tf.nn.relu(tf.add(tf.matmul(L1, W2), b2))\n",
    "L2 = tf.nn.dropout(_L2, dropout_rate)\n",
    "_L3 = tf.nn.relu(tf.add(tf.matmul(L2, W3), b3))\n",
    "L3 = tf.nn.dropout(_L3, dropout_rate)\n",
    "_L4 = tf.nn.relu(tf.add(tf.matmul(L3, W4), b4))\n",
    "L4 = tf.nn.dropout(_L4, dropout_rate)\n",
    "\n",
    "hypothesis = tf.add(tf.matmul(L4, W5), b5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cost and optimizer\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(hypothesis, Y))\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0001 cost= 0.475071080\n",
      "Epoch:  0002 cost= 0.190262621\n",
      "Epoch:  0003 cost= 0.143219065\n",
      "Epoch:  0004 cost= 0.115039476\n",
      "Epoch:  0005 cost= 0.098969297\n",
      "Epoch:  0006 cost= 0.088314197\n",
      "Epoch:  0007 cost= 0.079146385\n",
      "Epoch:  0008 cost= 0.072226720\n",
      "Epoch:  0009 cost= 0.067553020\n",
      "Epoch:  0010 cost= 0.059198628\n",
      "Epoch:  0011 cost= 0.058165367\n",
      "Epoch:  0012 cost= 0.050675838\n",
      "Epoch:  0013 cost= 0.050999129\n",
      "Epoch:  0014 cost= 0.047629396\n",
      "Epoch:  0015 cost= 0.044065304\n",
      "Epoch:  0016 cost= 0.041274765\n",
      "Epoch:  0017 cost= 0.039294836\n",
      "Epoch:  0018 cost= 0.041203228\n",
      "Epoch:  0019 cost= 0.036919394\n",
      "Epoch:  0020 cost= 0.037396075\n",
      "Epoch:  0021 cost= 0.033318758\n",
      "Epoch:  0022 cost= 0.036256944\n",
      "Epoch:  0023 cost= 0.032199785\n",
      "Epoch:  0024 cost= 0.029453857\n",
      "Epoch:  0025 cost= 0.032543999\n",
      "Optimization Finished\n",
      "Accuracy: 0.9816\n"
     ]
    }
   ],
   "source": [
    "# Launch\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Training\n",
    "    for epoch in range(training_epoch):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size) # 55000개 training data\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(100) # 1회에 100개씩 550번 loop\n",
    "            # Fit training using batch data\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={X : batch_xs, \n",
    "                                                          Y : batch_ys,\n",
    "                                                         dropout_rate:0.7})\n",
    "            # average loss\n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "        if epoch % 1 == 0:\n",
    "            print \"Epoch: \", \"%04d\" % (epoch + 1), \"cost=\", \"{:.9f}\".format(avg_cost)\n",
    "        \n",
    "    print \"Optimization Finished\"\n",
    "    \n",
    "    # Test\n",
    "    correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print \"Accuracy:\", accuracy.eval({X : mnist.test.images, \n",
    "                                      Y : mnist.test.labels, \n",
    "                                     dropout_rate: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'zeros_27:0' shape=(784, 10) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.zeros([784,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
